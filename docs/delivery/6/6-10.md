# [6-10] E2E CoS Testing

[Back to task list](./tasks.md)

## Description

Implement comprehensive End-to-End Conditions of Satisfaction (CoS) testing that validates all acceptance criteria from PBI 6. This final testing phase ensures the streaming engine meets all requirements specified in the PRD, including performance benchmarks, quality standards, multi-client support, synchronization, error handling, and resource management. These tests provide final validation that the PBI is complete and production-ready.

## Status History

| Timestamp | Event Type | From Status | To Status | Details | User |
|-----------|------------|-------------|-----------|---------|------|
| 2025-10-30 12:00:00 | Created | N/A | Proposed | Task file created | AI_Agent |

## Requirements

### Functional Requirements
- Validate all 22 acceptance criteria from PBI 6 PRD
- Test streaming with real video players (HLS.js, VLC)
- Verify performance requirements (startup <10s, 5+ concurrent streams)
- Test synchronization across multiple clients (±2 seconds)
- Verify all quality levels work correctly
- Test hardware acceleration with actual GPUs
- Validate error recovery mechanisms
- Verify resource cleanup prevents leaks
- Test on realistic hardware and network conditions

### Acceptance Criteria from PBI 6 PRD

**Functional Requirements:**
- [ ] HLS stream generation from current timeline position works correctly
- [ ] FFmpeg transcoding pipeline functional (H.264 + AAC output)
- [ ] Adaptive bitrate streams (1080p, 720p, 480p) generated correctly
- [ ] Hardware acceleration detection implemented for NVENC, QSV, VAAPI, VideoToolbox
- [ ] Hardware acceleration can be selected via configuration
- [ ] Fallback to software encoding when hardware unavailable
- [ ] Stream lifecycle management (start, stop, cleanup) working correctly
- [ ] Multi-client support: clients watching same channel share transcoding process
- [ ] Client count tracking per stream accurate
- [ ] Stream cleanup when last client disconnects (30-second grace period)
- [ ] Segment file cleanup prevents disk space exhaustion
- [ ] Synchronization across clients within ±2 seconds verified
- [ ] Stream startup time < 10 seconds
- [ ] System supports 5+ concurrent streams on test hardware
- [ ] Graceful error handling for FFmpeg crashes (restart attempt)
- [ ] Graceful error handling for missing/corrupted video files (skip to next)
- [ ] Graceful error handling for hardware encoder failures (fallback)
- [ ] Master playlist correctly lists all quality variants
- [ ] Media playlists update with new segments in real-time
- [ ] Segment files accessible via HTTP
- [ ] Integration tests with real video files pass
- [ ] Resource cleanup verified (no zombie FFmpeg processes)

## Implementation Plan

### Step 1: Set up E2E test environment
- Real database with sample channels and media
- Real video files (mix of formats, codecs, resolutions)
- Real FFmpeg with hardware acceleration available
- Test video player integration
- Performance monitoring tools

### Step 2: Test HLS generation and playback
```go
func TestE2E_HLSStreamGeneration(t *testing.T)
```

Verify:
1. Stream starts from correct timeline position
2. Master playlist generated with all quality variants
3. Media playlists generated for each quality
4. Segments generated continuously
5. Playback works in HLS.js test player
6. Adaptive bitrate switching works
7. Video quality matches configuration

### Step 3: Test FFmpeg transcoding pipeline
```go
func TestE2E_TranscodingPipeline(t *testing.T)
```

Verify:
1. Output is H.264 video codec
2. Output is AAC audio codec
3. Segments are valid MP2T format
4. All quality variants generated correctly
5. Bitrates match configuration (5000k, 3000k, 1500k)
6. Resolutions match configuration (1920x1080, 1280x720, 854x480)

### Step 4: Test hardware acceleration
```go
func TestE2E_HardwareAcceleration(t *testing.T)
```

Verify:
1. Detects available hardware encoders
2. Uses selected hardware encoder when configured
3. Configuration controls encoder selection
4. Fallback to software when hardware fails
5. Fallback to software when hardware unavailable
6. Performance improvement with hardware vs software

### Step 5: Test multi-client stream sharing
```go
func TestE2E_MultiClientSharing(t *testing.T)
```

Verify:
1. Multiple clients share single FFmpeg process
2. Client count tracking accurate
3. Stream continues with multiple clients
4. No duplicate segment generation
5. Bandwidth savings from sharing

### Step 6: Test stream lifecycle and cleanup
```go
func TestE2E_StreamLifecycle(t *testing.T)
```

Verify:
1. Stream starts when first client connects
2. Stream stops 30 seconds after last client disconnects
3. Segments cleaned up after stream stops
4. No zombie FFmpeg processes remain
5. No file descriptors leaked
6. No goroutines leaked
7. Memory returns to baseline after cleanup

### Step 7: Test synchronization across clients
```go
func TestE2E_ClientSynchronization(t *testing.T)
```

Verify:
1. Create stream with multiple clients
2. Capture playback position from each client
3. Verify positions within ±2 seconds
4. Test over time (5+ minutes)
5. Verify consistency across quality switches

### Step 8: Test performance requirements
```go
func TestE2E_Performance(t *testing.T)
```

Verify:
1. Stream startup time < 10 seconds (measure)
2. First segment available quickly (<5s)
3. 5+ concurrent streams supported on test hardware
4. CPU usage reasonable (<80% with software encoding)
5. Memory usage stable (no growth over time)
6. Network bandwidth usage as expected

### Step 9: Test error handling and recovery
```go
func TestE2E_ErrorHandling(t *testing.T)
```

Verify:
1. FFmpeg crash detected and restart attempted
2. Missing file skipped, stream continues with next
3. Corrupted file skipped, stream continues
4. Hardware encoder failure triggers software fallback
5. Disk space exhaustion handled gracefully
6. Error logs provide useful debugging information

### Step 10: Test resource management
```go
func TestE2E_ResourceManagement(t *testing.T)
```

Verify:
1. Segment cleanup prevents disk exhaustion
2. Old segments removed as new ones generated
3. Configurable segment retention (10 segments default)
4. Process cleanup on abnormal termination
5. Cleanup on server shutdown

### Step 11: Load and stress testing
```go
func TestE2E_LoadTesting(t *testing.T)
```

Verify:
1. 10 concurrent streams (if hardware supports)
2. 100 clients across 5 streams
3. Rapid client connect/disconnect
4. Stream start/stop cycling
5. System stability under load
6. Recovery after overload

### Step 12: Real player testing (manual)
- Test with HLS.js in browser
- Test with Video.js in browser
- Test with VLC media player
- Test with Safari native HLS
- Test on mobile devices
- Verify playback quality
- Verify seeking works
- Verify quality switching

## Test Plan

### Objective
Validate all acceptance criteria from PBI 6 PRD are met and system is production-ready.

### Test Execution
- Run automated E2E tests
- Perform manual testing with real players
- Conduct load testing on representative hardware
- Test on multiple operating systems (Linux, macOS)
- Test with various hardware acceleration options
- Document all test results

### Test Hardware
- Document test system specifications
- CPU, GPU, RAM, disk specifications
- Network configuration
- FFmpeg version and build options

### Success Criteria
- All 22 acceptance criteria validated
- Performance requirements met
- Real player testing successful
- Load testing demonstrates capacity
- No resource leaks detected
- System stable under stress
- Error recovery works as specified

## Verification

### Acceptance Criteria
- [ ] All 22 PRD acceptance criteria validated and passing
- [ ] Stream startup time measured and < 10 seconds
- [ ] 5+ concurrent streams tested and working
- [ ] Client synchronization measured within ±2 seconds
- [ ] Hardware acceleration tested on available hardware
- [ ] Software fallback verified when hardware unavailable
- [ ] Multi-client sharing reduces resource usage
- [ ] 30-second grace period verified
- [ ] Resource cleanup prevents leaks (processes, files, memory)
- [ ] Error handling tested for all specified scenarios
- [ ] Real player testing successful (HLS.js, VLC, Safari)
- [ ] Load testing demonstrates system capacity
- [ ] Performance benchmarks documented
- [ ] Test results documented with evidence
- [ ] All tests pass consistently
- [ ] Code compiles without errors
- [ ] No linter warnings

### Definition of Done
- All acceptance criteria validated with evidence
- Automated E2E tests pass
- Manual testing completed and documented
- Performance benchmarks met and documented
- Load testing results documented
- Test report created summarizing results
- Any issues documented and triaged
- PBI 6 ready for approval and closure

## Files Modified

### New Files Created
- `api/test/integration/streaming_e2e_test.go` - E2E CoS tests
- `api/test/integration/streaming_load_test.go` - Load and stress tests
- `docs/delivery/6/test-results.md` - Test results documentation

### Test Results Documentation
- Performance benchmark results
- Load testing capacity findings
- Hardware acceleration test results
- Real player compatibility results
- Resource usage measurements
- Error handling validation results

## Notes

- This is the final validation before PBI completion
- Tests should be thorough and cover all edge cases
- Performance measurements should be documented
- Hardware variations should be documented
- Test failures must be resolved before PBI completion
- Consider creating demo video of working system
- Real player testing validates client compatibility
- Load testing reveals system limits and bottlenecks
- Resource leak testing is critical for production readiness
- Document any known limitations or issues
- Test results become acceptance evidence
- May reveal issues requiring fixes in previous tasks
- Plan time for issue resolution if tests reveal problems
- Consider having stakeholder review demo
- Final milestone before marking PBI as Done
- Automated tests should run in CI for regression prevention
- Manual testing checklist ensures nothing missed
- Performance regression tests should be added to CI
- Consider recording test sessions for documentation
- Test on representative production hardware if possible

