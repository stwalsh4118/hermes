# [6-9] Integration Tests

[Back to task list](./tasks.md)

## Description

Implement comprehensive integration tests for the streaming engine that verify all components work together correctly with real FFmpeg execution and real video files. These tests validate the complete streaming pipeline from timeline calculation through FFmpeg transcoding to HLS segment generation, multi-client stream sharing, resource cleanup, and error recovery. Integration tests provide confidence that the streaming engine functions correctly in realistic scenarios.

## Status History

| Timestamp | Event Type | From Status | To Status | Details | User |
|-----------|------------|-------------|-----------|---------|------|
| 2025-10-30 12:00:00 | Created | N/A | Proposed | Task file created | AI_Agent |

## Requirements

### Functional Requirements
- Test complete streaming pipeline with real FFmpeg
- Verify HLS segment generation and playlists
- Test multi-client stream sharing
- Verify stream lifecycle (start, active, cleanup)
- Test with real video files in various formats
- Validate timeline integration
- Test hardware and software encoding paths
- Verify resource cleanup (no leaks)
- Test concurrent streams on different channels

### Technical Requirements
- Create `api/test/integration/streaming_test.go`
- Use real FFmpeg binary (must be installed)
- Create test video files or use fixtures
- Set up test database with channels and playlists
- Use real file system (with temp directories)
- Clean up all resources after tests
- Tests should be fast enough to run in CI (<2 minutes total)

### Test Fixtures Required

**Test Video Files:**
- Short test videos (10-30 seconds each)
- Multiple formats: MP4, MKV
- Various resolutions: 1080p, 720p, 480p
- Valid codecs (H.264, AAC)
- Small file sizes (<5MB each) for speed

**Test Database Setup:**
- Sample channels with different configurations
- Playlists with multiple media items
- Mix of looping and non-looping channels
- Various start times (past, current, future)

## Implementation Plan

### Step 1: Set up test infrastructure
- Create test helper functions
- Set up test database with fixtures
- Create or download test video files
- Set up temporary directories for segments
- Implement cleanup helper

### Step 2: Test basic stream lifecycle
```go
func TestStreamLifecycle(t *testing.T)
```

Steps:
1. Create test channel with playlist
2. Start stream via Stream Manager
3. Verify FFmpeg process launched
4. Wait for initial segments to generate
5. Verify master and media playlists exist
6. Verify segments files created
7. Stop stream
8. Verify FFmpeg process terminated
9. Verify segments cleaned up

### Step 3: Test multi-client stream sharing
```go
func TestMultiClientStreamSharing(t *testing.T)
```

Steps:
1. Create test channel
2. Register first client (starts stream)
3. Verify stream created
4. Register second client
5. Verify same stream shared (client count = 2)
6. Unregister first client
7. Verify stream still active (client count = 1)
8. Unregister second client
9. Verify grace period starts
10. Wait for grace period
11. Verify stream stopped and cleaned up

### Step 4: Test timeline integration
```go
func TestTimelineIntegration(t *testing.T)
```

Steps:
1. Create channel with known start time and playlist
2. Calculate expected timeline position
3. Start stream
4. Verify FFmpeg seek parameter matches timeline
5. Verify stream starts at correct position
6. Test with different timeline positions

### Step 5: Test quality variants
```go
func TestQualityVariants(t *testing.T)
```

Steps:
1. Start stream
2. Verify all three quality directories created (1080p, 720p, 480p)
3. Verify master playlist lists all variants
4. Verify each media playlist generated
5. Verify segments for each quality generated
6. Test accessing different quality streams

### Step 6: Test concurrent streams
```go
func TestConcurrentStreams(t *testing.T)
```

Steps:
1. Create multiple channels
2. Start streams on all channels simultaneously
3. Verify all streams active
4. Verify no resource conflicts
5. Verify all segments generating
6. Stop all streams
7. Verify all cleaned up

### Step 7: Test error scenarios
```go
func TestErrorRecovery(t *testing.T)
```

Scenarios:
- Missing media file (verify skip to next)
- FFmpeg crash (verify restart attempt)
- Hardware encoder unavailable (verify software fallback)
- Disk space issues (verify error handling)

### Step 8: Test grace period and cleanup
```go
func TestGracePeriodCleanup(t *testing.T)
```

Steps:
1. Start stream with client
2. Unregister client
3. Verify stream still active during grace period
4. Register client again before expiration
5. Verify stream stays active
6. Unregister and wait for expiration
7. Verify stream stopped and cleaned up

### Step 9: Performance and resource tests
```go
func TestStreamPerformance(t *testing.T)
```

Metrics:
- Stream startup time (<10 seconds)
- Segment generation rate (consistent)
- Memory usage (no leaks)
- Goroutine count (no leaks)
- File descriptor count (no leaks)

### Step 10: Create test helpers
```go
func setupTestChannel(t *testing.T) uuid.UUID
func setupTestPlaylist(t *testing.T, channelID uuid.UUID)
func waitForSegments(t *testing.T, streamPath string, count int)
func verifySegmentValid(t *testing.T, segmentPath string)
func verifyPlaylistValid(t *testing.T, playlistPath string)
func cleanupTestResources(t *testing.T, paths ...string)
```

## Test Plan

### Objective
Verify the complete streaming engine works correctly with real components and handles realistic scenarios.

### Test Scope
- Complete streaming pipeline
- Multi-client coordination
- Timeline integration
- Quality variants
- Concurrent streams
- Error recovery
- Resource cleanup
- Performance characteristics

### Test Execution
- Run in CI environment with FFmpeg installed
- Use parallel test execution where safe
- Set reasonable timeouts (30s per test)
- Use t.Cleanup() for resource cleanup
- Skip tests if FFmpeg not available (with clear message)

### Success Criteria
- All integration tests pass
- Tests run in <2 minutes total
- No resource leaks detected
- Tests are reliable (no flakes)
- Tests run in CI successfully
- Test coverage includes all major scenarios

## Verification

### Acceptance Criteria
- [ ] Test basic stream lifecycle (start, run, stop)
- [ ] Test multi-client stream sharing
- [ ] Test timeline integration with various positions
- [ ] Test all quality variants generated
- [ ] Test concurrent streams on different channels
- [ ] Test error recovery scenarios
- [ ] Test grace period and cleanup
- [ ] Test performance meets requirements
- [ ] Test helper functions simplify test writing
- [ ] All tests pass consistently
- [ ] Tests run in CI environment
- [ ] No resource leaks (processes, files, memory, goroutines)
- [ ] Tests are well-documented
- [ ] Test execution time acceptable (<2 minutes)
- [ ] Code compiles without errors
- [ ] No linter warnings

### Definition of Done
- All acceptance criteria met
- Comprehensive integration test suite
- Tests pass in CI environment
- Test fixtures documented and included
- Resource cleanup verified
- Performance benchmarks included
- Tests are maintainable and readable
- Test failures provide clear diagnostics
- Ready for E2E testing phase

## Files Modified

### New Files Created
- `api/test/integration/streaming_test.go` - Integration tests
- `api/test/integration/streaming_helpers.go` - Test helper functions
- `api/test/fixtures/video/` - Test video files (or download script)

### Test Fixtures
- Create or document test video files
- Include sample channels and playlists
- Document FFmpeg requirements for CI

## Notes

- Integration tests should run in CI with FFmpeg installed
- Tests should be fast enough to not slow down development workflow
- Use t.Parallel() carefully - some tests may conflict (disk I/O, ports)
- Test video files should be small but realistic
- Consider using generated test videos (FFmpeg can create them)
- Clean up all resources even if tests fail (use defer or t.Cleanup())
- Set timeouts to prevent hanging tests
- Log FFmpeg output for debugging test failures
- Consider table-driven tests for multiple scenarios
- Reference existing integration tests in `test/integration/`
- Tests should be deterministic (no race conditions)
- Use build tags to separate integration tests from unit tests (optional)
- Document FFmpeg version requirements for tests
- Consider mocking external services but use real FFmpeg
- Tests validate the PRD acceptance criteria
- May need to increase test timeouts on slow CI machines
- Consider parallel execution to speed up test suite
- Verify tests work on Linux, macOS, and Windows (if supported)

